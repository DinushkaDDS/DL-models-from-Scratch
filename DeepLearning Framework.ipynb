{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Below is the base data type class to store vectors/ matrices"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Tensor(object):\r\n",
    "\r\n",
    "    def __init__(self, data):\r\n",
    "        super().__init__()\r\n",
    "        self.data = np.array(data)\r\n",
    "\r\n",
    "    def __add__(self, other):\r\n",
    "        return Tensor(self.data + other.data)\r\n",
    "\r\n",
    "    def __repr__(self):\r\n",
    "        return str(self.data.__repr__())\r\n",
    "\r\n",
    "    def __str__(self):\r\n",
    "        return str(self.data.__str__())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "x = Tensor([1,2,3,4,5])\r\n",
    "print(x)\r\n",
    "\r\n",
    "y = x + x\r\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 2 3 4 5]\n",
      "[ 2  4  6  8 10]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### So in above if we want to add more functions to a tensor we need to create relavant functions inside of the class.\r\n",
    "> eg: add function\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In order to include automatic gradient computation we need to keep track of the functions performed on the Tensor and the graph of computations.\r\n",
    "\r\n",
    "### To do that we need to define few other properties to the above tensor class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class Tensor (object):\r\n",
    "    def __init__(self, data, creators=None, creation_op=None):\r\n",
    "        self.data = np.array(data)\r\n",
    "\r\n",
    "        # To keep track of the operation performed to the tensor.\r\n",
    "        self.creation_op = creation_op\r\n",
    "\r\n",
    "        # To keep track of graph of computation\r\n",
    "        self.creators = creators\r\n",
    "\r\n",
    "        # Gradient of this tensor\r\n",
    "        self.grad = None\r\n",
    "\r\n",
    "    # Backpropagation\r\n",
    "    def backward(self, grad):\r\n",
    "        # Given the input gradient we change the current one to that\r\n",
    "        self.grad = grad\r\n",
    "        if(self.creation_op == \"add\"):\r\n",
    "            # If operation if addition then linear propagation to the creator nodes\r\n",
    "            self.creators[0].backward(grad)\r\n",
    "            self.creators[1].backward(grad)\r\n",
    "        #print(self, self.grad)\r\n",
    "\r\n",
    "    def __add__(self, other):\r\n",
    "        return Tensor(self.data + other.data,\r\n",
    "                    creators=[self,other],\r\n",
    "                    creation_op=\"add\")\r\n",
    "\r\n",
    "    def __repr__(self):\r\n",
    "        return str(self.data.__repr__())\r\n",
    "\r\n",
    "    def __str__(self):\r\n",
    "        return str(self.data.__str__())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Below image shows the computational graph created by addition of 2 Tensors. This is how computational graphs are getting created.\r\n",
    "\r\n",
    "<center><img src=\"images\\computational_graph.png\" alt=\"Computational Graph\"></center>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "x = Tensor([1,2,3,4,5])\r\n",
    "y = Tensor([2,2,2,2,2])\r\n",
    "\r\n",
    "z = x + y\r\n",
    "z.backward(Tensor(np.array([1,1,1,1,1])))\r\n",
    "\r\n",
    "print(\"X\", x, x.grad)\r\n",
    "print(\"Y\", y, y.grad)\r\n",
    "print(\"Z\", z, z.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X [1 2 3 4 5] [1 1 1 1 1]\n",
      "Y [2 2 2 2 2] [1 1 1 1 1]\n",
      "Z [3 4 5 6 7] [1 1 1 1 1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Above implementation have a bug where if a Tensor get gradient updates from more than 2 paths gradient update would be wrong. (like in the below computation graph). In order to fix that instead of replacing the grad value we update it accordingly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class Tensor (object):\r\n",
    "    def __init__(self,data,\r\n",
    "                 autograd=False,\r\n",
    "                 creators=None,\r\n",
    "                 creation_op=None,\r\n",
    "                 id=None):  # ID to uniquely identify the Tensor\r\n",
    "\r\n",
    "        self.data = np.array(data)\r\n",
    "        self.creators = creators\r\n",
    "        self.creation_op = creation_op\r\n",
    "        self.grad = None\r\n",
    "        self.autograd = autograd\r\n",
    "        self.children = {} # To keep track of Tensor's child tensors\r\n",
    "\r\n",
    "        # Generate unique integer as an ID to the Tensor\r\n",
    "        if(id is None):\r\n",
    "            id = np.random.randint(0,100000)\r\n",
    "        self.id = id\r\n",
    "\r\n",
    "        # When creating a tensor, if creators are available, \r\n",
    "        #       we modify the creator children dictionary to keep track of newly created child.\r\n",
    "        if(creators is not None):\r\n",
    "            for c in creators:\r\n",
    "                if(self.id not in c.children):\r\n",
    "                    c.children[self.id] = 1\r\n",
    "                else:\r\n",
    "                    c.children[self.id] += 1\r\n",
    "\r\n",
    "    # Helper function to check whether all gradients have backpropagated from the child Tensors.\r\n",
    "    def all_children_grads_accounted_for(self):\r\n",
    "        for id, cnt in self.children.items():\r\n",
    "            if(cnt != 0):\r\n",
    "                return False\r\n",
    "        return True\r\n",
    "\r\n",
    "\r\n",
    "    def backward(self,grad=None, grad_origin=None):\r\n",
    "        if(self.autograd):\r\n",
    "            if(grad_origin is not None):\r\n",
    "                if(self.children[grad_origin.id] == 0):\r\n",
    "                    raise Exception(\"cannot backprop more than once\")\r\n",
    "                else:\r\n",
    "                    self.children[grad_origin.id] -= 1\r\n",
    "\r\n",
    "            # Accumulate gradients from all the paths and add them up\r\n",
    "            if(self.grad is None):\r\n",
    "                self.grad = grad\r\n",
    "            else:\r\n",
    "                self.grad += grad\r\n",
    "\r\n",
    "            if(self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):\r\n",
    "                if(self.creation_op == \"add\"):\r\n",
    "                    self.creators[0].backward(self.grad, self)\r\n",
    "                    self.creators[1].backward(self.grad, self)\r\n",
    "\r\n",
    "\r\n",
    "    def __add__(self, other):\r\n",
    "        if(self.autograd and other.autograd):\r\n",
    "            return Tensor(  self.data + other.data,\r\n",
    "                            autograd=True,\r\n",
    "                            creators=[self,other],\r\n",
    "                            creation_op=\"add\")\r\n",
    "        return Tensor(self.data + other.data)\r\n",
    "\r\n",
    "\r\n",
    "    def __repr__(self):\r\n",
    "        return str(self.data.__repr__())\r\n",
    "    def __str__(self):\r\n",
    "        return str(self.data.__str__())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src=\"images\\multipath_computational_graph.png\" alt=\"Computational Graph\"></center>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use the above image to understand the computatioanl graph and how the variable values change thorughout the process."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "a = Tensor([1,2,3,4,5], autograd=True)\r\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\r\n",
    "c = Tensor([5,4,3,2,1], autograd=True)\r\n",
    "\r\n",
    "d = a + b\r\n",
    "e = b + c\r\n",
    "f = d + e\r\n",
    "\r\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\r\n",
    "print(b.grad.data == np.array([2,2,2,2,2]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### If we need to add support to additional mathematical operations all we need to do is that adding the respective function and its derivative to the backpropagation logic. Check below for the implementation of \"Negation\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "class Tensor (object):\r\n",
    "    def __init__(self,data,\r\n",
    "                 autograd=False,\r\n",
    "                 creators=None,\r\n",
    "                 creation_op=None,\r\n",
    "                 id=None):  # ID to uniquely identify the Tensor\r\n",
    "\r\n",
    "        self.data = np.array(data)\r\n",
    "        self.creators = creators\r\n",
    "        self.creation_op = creation_op\r\n",
    "        self.grad = None\r\n",
    "        self.autograd = autograd\r\n",
    "        self.children = {} # To keep track of Tensor's child tensors\r\n",
    "\r\n",
    "        # Generate unique integer as an ID to the Tensor\r\n",
    "        if(id is None):\r\n",
    "            id = np.random.randint(0,100000)\r\n",
    "        self.id = id\r\n",
    "\r\n",
    "        # When creating a tensor, if creators are available, \r\n",
    "        #       we modify the creator children dictionary to keep track of newly created child.\r\n",
    "        if(creators is not None):\r\n",
    "            for c in creators:\r\n",
    "                if(self.id not in c.children):\r\n",
    "                    c.children[self.id] = 1\r\n",
    "                else:\r\n",
    "                    c.children[self.id] += 1\r\n",
    "\r\n",
    "    # Helper function to check whether all gradients have backpropagated from the child Tensors.\r\n",
    "    def all_children_grads_accounted_for(self):\r\n",
    "        for id, cnt in self.children.items():\r\n",
    "            if(cnt != 0):\r\n",
    "                return False\r\n",
    "        return True\r\n",
    "\r\n",
    "\r\n",
    "    def backward(self,grad=None, grad_origin=None):\r\n",
    "        if(self.autograd):\r\n",
    "            if(grad_origin is not None):\r\n",
    "                if(self.children[grad_origin.id] == 0):\r\n",
    "                    raise Exception(\"cannot backprop more than once\")\r\n",
    "                else:\r\n",
    "                    self.children[grad_origin.id] -= 1\r\n",
    "\r\n",
    "            # Accumulate gradients from all the paths and add them up\r\n",
    "            if(self.grad is None):\r\n",
    "                self.grad = grad\r\n",
    "            else:\r\n",
    "                self.grad += grad\r\n",
    "\r\n",
    "            if(self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):\r\n",
    "                if(self.creation_op == \"add\"):\r\n",
    "                    self.creators[0].backward(self.grad, self)\r\n",
    "                    self.creators[1].backward(self.grad, self)\r\n",
    "                if(self.creation_op == \"neg\"):\r\n",
    "                    # Taking the negation of the gradient tensor\r\n",
    "                    self.creators[0].backward(self.grad.__neg__())\r\n",
    "\r\n",
    "\r\n",
    "    def __add__(self, other):\r\n",
    "        if(self.autograd and other.autograd):\r\n",
    "            return Tensor(  self.data + other.data,\r\n",
    "                            autograd=True,\r\n",
    "                            creators=[self,other],\r\n",
    "                            creation_op=\"add\")\r\n",
    "        return Tensor(self.data + other.data)\r\n",
    "\r\n",
    "    def __neg__(self):\r\n",
    "        if(self.autograd):\r\n",
    "            return Tensor(  self.data * -1,\r\n",
    "                            autograd=True,\r\n",
    "                            creators=[self],\r\n",
    "                            creation_op=\"neg\")\r\n",
    "        return Tensor(self.data * -1)\r\n",
    "\r\n",
    "    def __repr__(self):\r\n",
    "        return str(self.data.__repr__())\r\n",
    "    def __str__(self):\r\n",
    "        return str(self.data.__str__())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "a = Tensor([1,2,3,4,5], autograd=True)\r\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\r\n",
    "c = Tensor([5,4,3,2,1], autograd=True)\r\n",
    "\r\n",
    "d = a + (-b)\r\n",
    "e = (-b) + c\r\n",
    "f = d + e\r\n",
    "\r\n",
    "# Note we are sending a Tensor as gradient\r\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\r\n",
    "print(b.grad.data == np.array([-2,-2,-2,-2,-2]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now below include Tensor class implementation with support for few other mathematical functions.\r\n",
    "* substraction\r\n",
    "* multiplication\r\n",
    "* sum (Sums values across a dimention)\r\n",
    "* expand (Function that copies data along a dimension)\r\n",
    "* transpose\r\n",
    "* mm (matrix multiplication)\r\n",
    "\r\n",
    "> Matrix Multiplication related backpropagation implementation is not intuitive. Therefore need to check after writing down the forward propagation steps."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "class Tensor (object):\r\n",
    "    def __init__(self,data,\r\n",
    "                 autograd=False,\r\n",
    "                 creators=None,\r\n",
    "                 creation_op=None,\r\n",
    "                 id=None):  # ID to uniquely identify the Tensor\r\n",
    "\r\n",
    "        self.data = np.array(data)\r\n",
    "        self.creators = creators\r\n",
    "        self.creation_op = creation_op\r\n",
    "        self.grad = None\r\n",
    "        self.autograd = autograd\r\n",
    "        self.children = {} # To keep track of Tensor's child tensors\r\n",
    "\r\n",
    "        # Generate unique integer as an ID to the Tensor\r\n",
    "        if(id is None):\r\n",
    "            id = np.random.randint(0,100000)\r\n",
    "        self.id = id\r\n",
    "\r\n",
    "        # When creating a tensor, if creators are available, \r\n",
    "        #       we modify the creator children dictionary to keep track of newly created child.\r\n",
    "        if(creators is not None):\r\n",
    "            for c in creators:\r\n",
    "                if(self.id not in c.children):\r\n",
    "                    c.children[self.id] = 1\r\n",
    "                else:\r\n",
    "                    c.children[self.id] += 1\r\n",
    "\r\n",
    "    # Helper function to check whether all gradients have backpropagated from the child Tensors.\r\n",
    "    def all_children_grads_accounted_for(self):\r\n",
    "        for id, cnt in self.children.items():\r\n",
    "            if(cnt != 0):\r\n",
    "                return False\r\n",
    "        return True\r\n",
    "\r\n",
    "\r\n",
    "    def backward(self,grad=None, grad_origin=None):\r\n",
    "        if(self.autograd):\r\n",
    "            if(grad_origin is not None):\r\n",
    "                if(self.children[grad_origin.id] == 0):\r\n",
    "                    raise Exception(\"cannot backprop more than once\")\r\n",
    "                else:\r\n",
    "                    self.children[grad_origin.id] -= 1\r\n",
    "\r\n",
    "            # Accumulate gradients from all the paths and add them up\r\n",
    "            if(self.grad is None):\r\n",
    "                self.grad = grad\r\n",
    "            else:\r\n",
    "                self.grad += grad\r\n",
    "\r\n",
    "            if(self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):\r\n",
    "                if(self.creation_op == \"add\"):\r\n",
    "                    self.creators[0].backward(self.grad, self)\r\n",
    "                    self.creators[1].backward(self.grad, self)\r\n",
    "\r\n",
    "                if(self.creation_op == \"neg\"):\r\n",
    "                    # Taking the negation of the gradient tensor\r\n",
    "                    self.creators[0].backward(self.grad.__neg__())\r\n",
    "\r\n",
    "                if(self.creation_op == \"sub\"):\r\n",
    "                    new = Tensor(self.grad.data)\r\n",
    "                    self.creators[0].backward(new, self)\r\n",
    "                    new = Tensor(self.grad.__neg__().data)\r\n",
    "                    self.creators[1].backward(new, self)\r\n",
    "\r\n",
    "                if(self.creation_op == \"mul\"):\r\n",
    "                    new = self.grad * self.creators[1]\r\n",
    "                    self.creators[0].backward(new , self)\r\n",
    "                    new = self.grad * self.creators[0]\r\n",
    "                    self.creators[1].backward(new, self)\r\n",
    "\r\n",
    "                if(self.creation_op == \"mm\"):\r\n",
    "                    act = self.creators[0]\r\n",
    "                    weights = self.creators[1]\r\n",
    "\r\n",
    "                    # This isequivalent to --> layer_1_delta=layer_2_delta.dot(weights_1_2.T) part\r\n",
    "                    new = self.grad.mm(weights.transpose()) \r\n",
    "                    act.backward(new)\r\n",
    "                    new = self.grad.transpose().mm(act).transpose()\r\n",
    "                    weights.backward(new)\r\n",
    "\r\n",
    "                if(self.creation_op == \"transpose\"):\r\n",
    "                    self.creators[0].backward(self.grad.transpose())\r\n",
    "\r\n",
    "                if(\"sum\" in self.creation_op):\r\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\r\n",
    "                    ds = self.creators[0].data.shape[dim]\r\n",
    "                    self.creators[0].backward(self.grad.expand(dim,ds))\r\n",
    "\r\n",
    "                if(\"expand\" in self.creation_op):\r\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\r\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\r\n",
    "\r\n",
    "\r\n",
    "    def __add__(self, other):\r\n",
    "        if(self.autograd and other.autograd):\r\n",
    "            return Tensor(  self.data + other.data,\r\n",
    "                            autograd=True,\r\n",
    "                            creators=[self,other],\r\n",
    "                            creation_op=\"add\")\r\n",
    "        return Tensor(self.data + other.data)\r\n",
    "\r\n",
    "    def __neg__(self):\r\n",
    "        if(self.autograd):\r\n",
    "            return Tensor(  self.data * -1,\r\n",
    "                            autograd=True,\r\n",
    "                            creators=[self],\r\n",
    "                            creation_op=\"neg\")\r\n",
    "        return Tensor(self.data * -1)\r\n",
    "\r\n",
    "    def __sub__(self, other):\r\n",
    "        if(self.autograd and other.autograd):\r\n",
    "            return Tensor(  self.data - other.data,\r\n",
    "                            autograd=True,\r\n",
    "                            creators=[self,other],\r\n",
    "                            creation_op=\"sub\")\r\n",
    "        return Tensor(self.data - other.data)\r\n",
    "\r\n",
    "    def __mul__(self, other):\r\n",
    "        if(self.autograd and other.autograd):\r\n",
    "            return Tensor(  self.data * other.data,\r\n",
    "                            autograd=True,\r\n",
    "                            creators=[self,other],\r\n",
    "                            creation_op=\"mul\")\r\n",
    "        return Tensor(self.data * other.data)\r\n",
    "\r\n",
    "    def sum(self, dim):\r\n",
    "        if(self.autograd):\r\n",
    "            return Tensor(  self.data.sum(dim), # Getting the sum over desired dimension\r\n",
    "                            autograd=True,\r\n",
    "                            creators=[self],\r\n",
    "                            creation_op=\"sum_\" + str(dim))\r\n",
    "        return Tensor(self.data.sum(dim))\r\n",
    "\r\n",
    "    def expand(self, dim, copies):\r\n",
    "        trans_cmd = list(range(0, len(self.data.shape)))\r\n",
    "        trans_cmd.insert(dim, len(self.data.shape))\r\n",
    "\r\n",
    "        new_shape = list(self.data.shape) + [copies]\r\n",
    "\r\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\r\n",
    "        new_data = new_data.transpose(trans_cmd)\r\n",
    "\r\n",
    "        if(self.autograd):\r\n",
    "            return Tensor(  new_data,\r\n",
    "                            autograd=True,\r\n",
    "                            creators=[self],\r\n",
    "                            creation_op=\"expand_\"+str(dim))\r\n",
    "        return Tensor(new_data)\r\n",
    "\r\n",
    "    def transpose(self):\r\n",
    "        if(self.autograd):\r\n",
    "            return Tensor(  self.data.transpose(),\r\n",
    "                            autograd=True,\r\n",
    "                            creators=[self],\r\n",
    "                            creation_op=\"transpose\")\r\n",
    "        return Tensor(self.data.transpose())\r\n",
    "\r\n",
    "    def mm(self, x):\r\n",
    "        if(self.autograd):\r\n",
    "            return Tensor(  self.data.dot(x.data),\r\n",
    "                            autograd=True,\r\n",
    "                            creators=[self,x],\r\n",
    "                            creation_op=\"mm\")\r\n",
    "        return Tensor(self.data.dot(x.data))\r\n",
    "\r\n",
    "\r\n",
    "    def __repr__(self):\r\n",
    "        return str(self.data.__repr__())\r\n",
    "    def __str__(self):\r\n",
    "        return str(self.data.__str__())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now we can compare the original vs framework based implementation of a Neural Network\r\n",
    "\r\n",
    "> ### Regular implementation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "np.random.seed(0)\r\n",
    "\r\n",
    "data = np.array([[0,0],[0,1],[1,0],[1,1]])\r\n",
    "target = np.array([[0],[1],[0],[1]])\r\n",
    "\r\n",
    "weights_0_1 = np.random.rand(2,3)\r\n",
    "weights_1_2 = np.random.rand(3,1)\r\n",
    "\r\n",
    "for i in range(10):\r\n",
    "    # Forward pass\r\n",
    "    layer_1 = data.dot(weights_0_1)\r\n",
    "    layer_2 = layer_1.dot(weights_1_2)\r\n",
    "\r\n",
    "    # Loss calculation\r\n",
    "    diff = (layer_2 - target)\r\n",
    "    sqdiff = (diff * diff)\r\n",
    "    loss = sqdiff.sum(0)\r\n",
    "\r\n",
    "    # Backpropagation\r\n",
    "    layer_1_grad = diff.dot(weights_1_2.transpose())\r\n",
    "    weight_1_2_update = layer_1.transpose().dot(diff)\r\n",
    "    weight_0_1_update = data.transpose().dot(layer_1_grad)\r\n",
    "\r\n",
    "    # Weight Updation\r\n",
    "    weights_1_2 -= weight_1_2_update * 0.1\r\n",
    "    weights_0_1 -= weight_0_1_update * 0.1\r\n",
    "    print(loss[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.066439994622396\n",
      "0.4959907791902341\n",
      "0.4180671892167177\n",
      "0.35298133007809646\n",
      "0.2972549636567376\n",
      "0.24923260381633278\n",
      "0.20785392075862477\n",
      "0.17231260916265181\n",
      "0.14193744536652994\n",
      "0.11613979792168387\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Framework based implementation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "np.random.seed(0)\r\n",
    "\r\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\r\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\r\n",
    "\r\n",
    "w = list()\r\n",
    "w.append(Tensor(np.random.rand(2,3), autograd=True))\r\n",
    "w.append(Tensor(np.random.rand(3,1), autograd=True))\r\n",
    "\r\n",
    "for i in range(10):\r\n",
    "    # Forward pass\r\n",
    "    pred = data.mm(w[0]).mm(w[1])\r\n",
    "\r\n",
    "    # Calculate Loss\r\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\r\n",
    "\r\n",
    "    # back propagation calculation\r\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\r\n",
    "\r\n",
    "    # Weight Updation\r\n",
    "    for w_ in w:\r\n",
    "        w_.data -= w_.grad.data * 0.1\r\n",
    "        w_.grad.data *= 0\r\n",
    "\r\n",
    "    print(loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.58128304]\n",
      "[0.48988149]\n",
      "[0.41375111]\n",
      "[0.34489412]\n",
      "[0.28210124]\n",
      "[0.2254484]\n",
      "[0.17538853]\n",
      "[0.1324231]\n",
      "[0.09682769]\n",
      "[0.06849361]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "f1058ac39a4b5cc6a2d664bf07a90cc7a0b869b1d28e3e4a0289bda448411850"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}