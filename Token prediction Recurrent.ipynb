{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys, random, math\r\n",
    "from collections import Counter\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "f = open('datasets/tasksv11/en/qa1_single-supporting-fact_train.txt','r')\r\n",
    "raw = f.readlines()\r\n",
    "f.close()\r\n",
    "\r\n",
    "tokens = list()\r\n",
    "for line in raw[0:1000]:\r\n",
    "    temp = []\r\n",
    "    for i in line.lower().replace(\"\\n\",\"\").replace(\"\\t\", \" \").split(\" \")[:]:\r\n",
    "        if(i != \"\" and not i.isdigit()):\r\n",
    "            temp.append(i)\r\n",
    "\r\n",
    "    tokens.append(temp)\r\n",
    "\r\n",
    "print(tokens[0:3])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['mary', 'moved', 'to', 'the', 'bathroom.'], ['john', 'went', 'to', 'the', 'hallway.'], ['where', 'is', 'mary?', 'bathroom']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "vocab = set()\r\n",
    "for sent in tokens:\r\n",
    "    for word in sent:\r\n",
    "        vocab.add(word)\r\n",
    "vocab = list(vocab)\r\n",
    "\r\n",
    "\r\n",
    "word2index = {}\r\n",
    "for i,word in enumerate(vocab):\r\n",
    "    word2index[word]=i\r\n",
    "\r\n",
    "def words2indices(sentence):\r\n",
    "    idx = list()\r\n",
    "    for word in sentence:\r\n",
    "        idx.append(word2index[word])\r\n",
    "    return idx\r\n",
    "\r\n",
    "def softmax(x):\r\n",
    "    e_x = np.exp(x - np.max(x))\r\n",
    "    return e_x / e_x.sum(axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "np.random.seed(1)\r\n",
    "\r\n",
    "embed_size = 10\r\n",
    "embed = (np.random.rand(len(vocab), embed_size) - 0.5) * 0.1\r\n",
    "\r\n",
    "recurrent = np.eye(embed_size)\r\n",
    "\r\n",
    "# Initial Input layer\r\n",
    "start = np.zeros(embed_size)\r\n",
    "\r\n",
    "# Final Layer\r\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1\r\n",
    "\r\n",
    "# Smart way of creating a one hot encodeded values for tokens\r\n",
    "one_hot = np.eye(len(vocab))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(vocab)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['hallway.', 'moved', 'daniel', 'bathroom', 'travelled', 'the', 'where', 'journeyed', 'kitchen.', 'john', 'to', 'office', 'back', 'bedroom', 'bathroom.', 'mary', 'is', 'daniel?', 'hallway', 'bedroom.', 'kitchen', 'went', 'john?', 'garden', 'mary?', 'sandra', 'sandra?', 'office.', 'garden.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def predict(sent):\r\n",
    "    '''\r\n",
    "    sent is a list with token indexes from the vocabulary.\r\n",
    "    eg [15, 56, 561, 35432, 321, 5468]\r\n",
    "    '''\r\n",
    "\r\n",
    "    layers = list()\r\n",
    "    layer = {}\r\n",
    "\r\n",
    "    # Hidden layer setup to previously defined embedded layer\r\n",
    "    layer['hidden'] = start\r\n",
    "    layers.append(layer)\r\n",
    "\r\n",
    "    loss = 0\r\n",
    "    preds = list()\r\n",
    "\r\n",
    "    # Running a loop for each token in a sentence\r\n",
    "    for target_i in range(len(sent)):\r\n",
    "        layer = {}\r\n",
    "\r\n",
    "        # Predicting which word is going to come up from the vocabulary\r\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))\r\n",
    "\r\n",
    "        # From layer[\"pred\"] of dim (len(vocab), 1) get the actual required value vector \r\n",
    "        # if it is 1 then loss is zero, else we get a value because of log transformation\r\n",
    "        loss += -np.log(layer['pred'][sent[target_i]])\r\n",
    "        \r\n",
    "        # Similar to ((embeded vector*identity matrix) + next embeded vector) calculation\r\n",
    "        # This part is responsible to keep the sequence factor.\r\n",
    "        # Check the above image token vector multiplacation.\r\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[sent[target_i]]\r\n",
    "\r\n",
    "        layers.append(layer)\r\n",
    "        \r\n",
    "    return layers, loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./images/sentence_embedding.png\" width=400 height=400 />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for iter in range(30000):\r\n",
    "    alpha = 0.001\r\n",
    "\r\n",
    "    # Getting the sentence as vector of indexes\r\n",
    "    sent = words2indices(tokens[iter%len(tokens)][1:])\r\n",
    "    \r\n",
    "    # do the forward propagation for the respective sentence\r\n",
    "    layers, loss = predict(sent)\r\n",
    "\r\n",
    "    for layer_idx in reversed(range(len(layers))):\r\n",
    "\r\n",
    "        # Select the token related weight values\r\n",
    "        layer = layers[layer_idx]\r\n",
    "\r\n",
    "        # Select the target token\r\n",
    "        target = sent[layer_idx-1]\r\n",
    "\r\n",
    "        # If not the First layer\r\n",
    "        if(layer_idx > 0):\r\n",
    "            \r\n",
    "            # Take the error between output and target\r\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\r\n",
    "            \r\n",
    "            # calculate the (1, embed_size) delta vector for the embed layer\r\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\r\n",
    "\r\n",
    "            if(layer_idx == len(layers)-1):\r\n",
    "                # If this is the last layer, then no other derivatives to be add up from recurrent part\r\n",
    "                layer['hidden_delta'] = new_hidden_delta\r\n",
    "            else:\r\n",
    "                # this is an intermediate layer\r\n",
    "                # So we need to calculate the recurrent derivative from its higher layer\r\n",
    "                layer['hidden_delta'] = new_hidden_delta + \\\r\n",
    "                            layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\r\n",
    "        \r\n",
    "        # If the first layer\r\n",
    "        else: \r\n",
    "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\r\n",
    "\r\n",
    "\r\n",
    "    # Weight Update part in backpropagation\r\n",
    "    start -= layers[0]['hidden_delta'] * alpha / float(len(sent))\r\n",
    "\r\n",
    "    for layer_idx, layer in enumerate(layers[1:]):\r\n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) * alpha / float(len(sent))\r\n",
    "        \r\n",
    "        embed_idx = sent[layer_idx]\r\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * alpha / float(len(sent))\r\n",
    "\r\n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * alpha / float(len(sent))\r\n",
    "\r\n",
    "\r\n",
    "    if(iter % 1000 == 0):\r\n",
    "        print(\"Perplexity:\" + str(np.exp(loss/len(sent))))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Perplexity:28.942281525605402\n",
      "Perplexity:28.925850840833487\n",
      "Perplexity:28.904242713024555\n",
      "Perplexity:28.871623793014024\n",
      "Perplexity:28.818741249448134\n",
      "Perplexity:28.729289017132416\n",
      "Perplexity:28.571491359143295\n",
      "Perplexity:28.274319738835416\n",
      "Perplexity:27.639781956222553\n",
      "Perplexity:25.85997851495855\n",
      "Perplexity:20.71041578929764\n",
      "Perplexity:18.175655002034116\n",
      "Perplexity:16.238628070027822\n",
      "Perplexity:15.15846919321447\n",
      "Perplexity:14.607433816143516\n",
      "Perplexity:13.981369537512515\n",
      "Perplexity:12.778941883352104\n",
      "Perplexity:10.520162722942732\n",
      "Perplexity:8.472425345384403\n",
      "Perplexity:7.6433799370600815\n",
      "Perplexity:7.291982765285883\n",
      "Perplexity:7.0498710817537145\n",
      "Perplexity:6.836422204250653\n",
      "Perplexity:6.64006635389034\n",
      "Perplexity:6.459662355419034\n",
      "Perplexity:6.294923308640655\n",
      "Perplexity:6.1444986803329265\n",
      "Perplexity:6.006583422348548\n",
      "Perplexity:5.876510208041621\n",
      "Perplexity:5.74833899576789\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "sent_index = 4\r\n",
    "\r\n",
    "l, _ = predict(words2indices(tokens[sent_index]))\r\n",
    "print(tokens[sent_index])\r\n",
    "\r\n",
    "for i, each_layer in enumerate(l[1:-1]):\r\n",
    "    input = tokens[sent_index][i]\r\n",
    "    true = tokens[sent_index][i+1]\r\n",
    "    pred = vocab[each_layer['pred'].argmax()]\r\n",
    "    print(\"Prev Input:\" + input + (' ' * (12 - len(input))) +\\\r\n",
    "    \"True:\" + true + (\" \" * (15 - len(true))) + \"Pred:\" + pred)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['sandra', 'moved', 'to', 'the', 'garden.']\n",
      "Prev Input:sandra      True:moved          Pred:is\n",
      "Prev Input:moved       True:to             Pred:to\n",
      "Prev Input:to          True:the            Pred:to\n",
      "Prev Input:the         True:garden.        Pred:the\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "f1058ac39a4b5cc6a2d664bf07a90cc7a0b869b1d28e3e4a0289bda448411850"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}