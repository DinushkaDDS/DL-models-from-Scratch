{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "def softmax(x_):\r\n",
    "    x = np.atleast_2d(x_)\r\n",
    "    temp = np.exp(x)\r\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\r\n",
    "\r\n",
    "# Lets consider a limited vocabulary of 9 words\r\n",
    "word_vects = {}\r\n",
    "word_vects['yankees'] = np.array([[0.,0.,0.]])\r\n",
    "word_vects['bears'] = np.array([[0.,0.,0.]])\r\n",
    "word_vects['braves'] = np.array([[0.,0.,0.]])\r\n",
    "word_vects['red'] = np.array([[0.,0.,0.]])\r\n",
    "word_vects['sox'] = np.array([[0.,0.,0.]])\r\n",
    "word_vects['lose'] = np.array([[0.,0.,0.]])\r\n",
    "word_vects['defeat'] = np.array([[0.,0.,0.]])\r\n",
    "word_vects['beat'] = np.array([[0.,0.,0.]])\r\n",
    "word_vects['tie'] = np.array([[0.,0.,0.]])\r\n",
    "\r\n",
    "# Classification layer (Weight matrix to predict next word given a sentence vector of size 3)\r\n",
    "sent2output = np.random.rand(3, len(word_vects))\r\n",
    "\r\n",
    "# Transition weights (Identity matix initially)\r\n",
    "identity = np.eye(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(sent2output)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.01636224 0.08583247 0.15471846 0.85304228 0.62501893 0.48109461\n",
      "  0.55273576 0.50447677 0.56604863]\n",
      " [0.22439237 0.75110089 0.67995706 0.1248642  0.89321913 0.85559151\n",
      "  0.83948413 0.01708117 0.19405993]\n",
      " [0.32043313 0.14318351 0.83059157 0.34954823 0.08050018 0.18747478\n",
      "  0.74834337 0.15884757 0.10447104]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\r\n",
    "# Forward propagation for the sentence \"red sox defeat\" to predict \"yankees\".\r\n",
    "layer_0 = word_vects['red']\r\n",
    "layer_1 = layer_0.dot(identity) + word_vects['sox']\r\n",
    "layer_2 = layer_1.dot(identity) + word_vects['defeat']\r\n",
    "\r\n",
    "# Trying to predict which values should come up given previous inputs\r\n",
    "pred = softmax(layer_2.dot(sent2output))\r\n",
    "\r\n",
    "print(pred)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "  0.11111111 0.11111111 0.11111111]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./images/sentence_embedding.png\" width=400 height=400 />\r\n",
    "\r\n",
    "### Image credit: Grokkin Deep Learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# This is the expected output \"Yankees\" (from the classification layer)\r\n",
    "y = np.array([1,0,0,0,0,0,0,0,0])\r\n",
    "\r\n",
    "# Doing the backpropagation\r\n",
    "pred_delta = pred - y   # Error\r\n",
    "\r\n",
    "layer_2_delta = pred_delta.dot(sent2output.T)\r\n",
    "\r\n",
    "defeat_delta = layer_2_delta * 1\r\n",
    "layer_1_delta = layer_2_delta.dot(identity.T)\r\n",
    "\r\n",
    "sox_delta = layer_1_delta * 1\r\n",
    "layer_0_delta = layer_1_delta.dot(identity.T)\r\n",
    "\r\n",
    "alpha = 0.01\r\n",
    "\r\n",
    "# Modifing the word embedding based on the gradient\r\n",
    "word_vects['red'] -= layer_0_delta * alpha\r\n",
    "word_vects['sox'] -= sox_delta * alpha\r\n",
    "word_vects['defeat'] -= defeat_delta * alpha\r\n",
    "\r\n",
    "# Modifying the transition matrix\r\n",
    "identity -= np.outer(layer_0, layer_1_delta) * alpha\r\n",
    "identity -= np.outer(layer_1, layer_2_delta) * alpha\r\n",
    "\r\n",
    "sent2output -= np.outer(layer_2, pred_delta) * alpha\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.41023    0.28446878 0.00438836]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### When you add two vectors together during forward propagation, you backpropagate the same gradient into both sides of the addition. When you generate **layer_2_delta**, youâ€™ll backpropagate it twice: once across the identity matrix to create **layer_1_delta**, and again to **word_vects['defeat']**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "f1058ac39a4b5cc6a2d664bf07a90cc7a0b869b1d28e3e4a0289bda448411850"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}